from pydantic import BaseModel, Field, field_validator, ValidationError
from enum import Enum
from datetime import datetime, timezone
from typing import Optional, List, Any
import uuid


class ModelProvider(str, Enum):
    OPENAI = "openai"
    GOOGLE = "google"
    GROQ = "groq"


class ModelName(str, Enum):
    """
    Enum representing supported language models across multiple providers.

    Models are categorized by provider:
    - OpenAI: GPT-5 series
    - Google: Gemini 2.5 models
    - Groq Cloud: LLaMA3.1/3.3 variants
    """
    # OpenAI Models
    GPT_5 = "gpt-5"
    GPT_5_MINI = "gpt-5-mini"
    GPT_NANO = "gpt-nano"

    # Google Gemini Models (latest only)
    GEMINI_2_5_PRO = "gemini-2.5-pro"
    GEMINI_2_5_FLASH = "gemini-2.5-flash"
    GEMINI_2_5_FLASH_LITE = "gemini-2.5-flash-lite"

    # Groq Cloud Models
    LLAMA_3_1_8B_INSTANT = "llama-3.1-8b-instant"
    LLAMA_3_3_70B_VERSATILE = "llama-3.3-70b-versatile"

    def get_provider(self) -> ModelProvider:
        """
        Returns the provider associated with the model name.

        This helper method infers the provider (OpenAI, Google, Groq)
        by inspecting the model's value. This is useful for dynamically
        routing model usage logic based on source.

        Returns:
            ModelProvider: The corresponding provider enum.

        Raises:
            ValueError: If the provider cannot be determined from the model name.
        """
        value_lower = self.value.lower()
        if value_lower.startswith("gpt"):
            return ModelProvider.OPENAI
        elif value_lower.startswith("gemini"):
            return ModelProvider.GOOGLE
        elif "llama" in value_lower:  # Simplified for the Groq models we have
            return ModelProvider.GROQ
        raise ValueError(
            f"Could not determine provider for model: {self.value}")


# Helper model for chat history input in the API
class ChatMessageAPI(BaseModel):
    role: str = Field(..., examples=["user", "assistant"])
    content: str

    @field_validator('role')
    @classmethod
    def role_must_be_valid(cls, v: str) -> str:
        role_lower = v.lower()
        # Allow more for flexibility
        if role_lower not in ["user", "assistant", "system", "human", "ai"]:
            raise ValueError(
                "Role must be one of 'user', 'assistant', 'system', 'human', 'ai'")
        return role_lower

# class QueryInputForComparison(BaseModel):
#     """
#     Request model for submitting a query to the LLM comparison endpoint.

#     This model is used to encapsulate the user's input question and an optional session ID.
#     The session ID allows grouping multiple requests into a single logical session for
#     tracking, analysis, or UI display purposes.

#     Attributes:
#         question (str): The input question or prompt to be compared across different LLM providers.
#         session_id (Optional[str]): A unique identifier for the session. If not provided, a new UUID
#                                     is automatically generated to uniquely identify the request context.
#     """
#     question: str
#     session_id: Optional[str] = Field(default_factory=lambda: str(uuid.uuid4()))

#     @field_validator('session_id', mode='before') # (B)
#     @classmethod
#     def set_session_id_if_none(cls, v):
#         return v or str(uuid.uuid4())

# Request body for individual LLM chat endpoints


class SingleModelChatRequest(BaseModel):
    question: str = Field(..., examples=["What is the capital of France?"])
    session_id: Optional[str] = Field(
        default_factory=lambda: str(uuid.uuid4()))
    chat_history: Optional[List[ChatMessageAPI]] = Field(
        default_factory=list,
        examples=[[{"role": "user", "content": "My name is Bob."}]]
    )
    system_prompts: Optional[List[str]] = Field(
        default_factory=list,
        description="List of system prompts to prepend to the chat history for context"
    )

    @field_validator('session_id', mode='before')
    @classmethod
    def set_session_id_if_none(cls, v: Optional[str]) -> str:
        return v or str(uuid.uuid4())


class QueryResponse(BaseModel):
    """
    Represents a single response returned by an AI model in a comparison session.

    This model is used to structure the metadata and result of a model's response to a query.
    It includes timing information, the model identity, and the associated session ID.

    Attributes:
        answer (str): The textual response generated by the AI model.
        session_id (str): Unique identifier for the comparison session this response belongs to.
        model (ModelName): The specific model that generated the response (e.g., GPT-4o, Gemini).
        provider (ModelProvider): The provider associated with the model (e.g., OpenAI, Google, Groq).
        request_timestamp (datetime): The timestamp when the query was sent to the model.
        response_timestamp (datetime): The timestamp when the response was received from the model.
        latency_ms (float): The total time taken to receive the model's response, in milliseconds.
        usage (Optional[Any]): Token usage information from the LLM provider (format varies by provider).
    """
    answer: Optional[str] = None
    raw_response: Optional[Any] = None
    error_message: Optional[str] = None
    session_id: str
    model: ModelName
    provider: ModelProvider
    request_timestamp: datetime
    response_timestamp: datetime
    latency_ms: float
    usage: Optional[Any] = None  # Token usage info (varies by provider)

    @field_validator('request_timestamp', 'response_timestamp', mode='before')
    @classmethod
    # Use `any` for `v` if it might be a string from JSON
    def ensure_datetime_is_aware(cls, v: datetime) -> datetime:
        """
        Validates that the datetime field is timezone-aware.
        This runs AFTER Pydantic has converted the input to a datetime object.
        """
        if v.tzinfo is None:
            raise ValueError("Datetime must be timezone-aware (UTC).")
        return v.astimezone(timezone.utc)


class ComparisonResponse(BaseModel):
    """
    Represents the collection of responses from multiple language models for a single input query.

    This model is used to return all model responses for a given question, grouped under a shared
    session ID. It includes metadata such as the original user question and the timestamp when the
    comparison was performed.

    Attributes:
        original_question (str): The user-provided question that was submitted to all models.
        session_id (str): Unique identifier used to group all model responses under a single session.
        responses (List[QueryResponse]): A list of individual responses from each model involved in the comparison.
        comparison_timestamp (datetime): The timestamp when the comparison was completed. Defaults to the current UTC time.
    """
    original_question: str
    session_id: str
    responses: List[QueryResponse]
    comparison_timestamp: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc))
